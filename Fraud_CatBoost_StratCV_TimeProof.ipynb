{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve"
      ],
      "metadata": {
        "id": "RnUi6UPPGd5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering utilities"
      ],
      "metadata": {
        "id": "cBrRd877Gphv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Transctional Features\n",
        "# -----------------------------\n",
        "\n",
        "def create_base_features(df):\n",
        "    \"\"\"Basic transaction + demographic derived features.\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Ensure datetime\n",
        "    df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
        "\n",
        "    # Per-customer aggregates (global / history)\n",
        "    cust_aggs = df.groupby('customer_id').agg(\n",
        "        total_txns=('transaction_id','nunique'),\n",
        "        first_txn=('transaction_date','min'),\n",
        "        last_txn=('transaction_date','max'),\n",
        "        avg_txn_amount=('amount','mean'),\n",
        "        std_txn_amount=('amount','std'),\n",
        "        max_txn_amount=('amount','max')\n",
        "    ).reset_index()\n",
        "    cust_aggs['cust_active_days'] = (cust_aggs['last_txn'] - cust_aggs['first_txn']).dt.days + 1\n",
        "    cust_aggs['txns_per_day'] = cust_aggs['total_txns'] / cust_aggs['cust_active_days'].replace(0,1)\n",
        "\n",
        "    df = df.merge(cust_aggs, on='customer_id', how='left')\n",
        "\n",
        "    # Recency / frequency / monetary for the specific transaction\n",
        "    latest_date = df['transaction_date'].max()\n",
        "    df['days_since_last_txn'] = (latest_date - df['transaction_date']).dt.days\n",
        "    df['amount_over_avg'] = df['amount'] / (df['avg_txn_amount'] + 1e-9)\n",
        "\n",
        "    # Time features\n",
        "    df['hour'] = df['transaction_date'].dt.hour\n",
        "    df['dayofweek'] = df['transaction_date'].dt.dayofweek\n",
        "    df['is_weekend'] = df['dayofweek'].isin([5,6]).astype(int)\n",
        "\n",
        "    # Demographics\n",
        "    if 'age' in df.columns:\n",
        "        df['age_bucket'] = pd.cut(df['age'], bins=[0,25,35,50,65,200], labels=False)\n",
        "    if 'income' in df.columns:\n",
        "        df['log_income'] = np.log1p(df['income'])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "XdlraY-sGobd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Rolling (Txn) Features\n",
        "# -----------------------------\n",
        "\n",
        "def create_rolling_features(df, windows=[7,30,90]):\n",
        "    \"\"\"Creates rolling-window features per customer (requires transaction-level ordered by date).\"\"\"\n",
        "    df = df.sort_values(['customer_id','transaction_date'])\n",
        "    for w in windows:\n",
        "        key = f'txn_count_{w}d'\n",
        "        df[key] = df.groupby('customer_id')['transaction_id'].transform(lambda x: x.rolling(w, min_periods=1).count())\n",
        "\n",
        "        amt_key = f'txn_amt_mean_{w}d'\n",
        "        df[amt_key] = df.groupby('customer_id')['amount'].transform(lambda x: x.rolling(w, min_periods=1).mean())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "8cnZpbO0G8I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Behavioral Features\n",
        "# -----------------------------\n",
        "\n",
        "def behavioral_features(df):\n",
        "    \"\"\"Create behavioral signals: velocity, unusual amount zscore, high-risk ratios.\"\"\"\n",
        "    df = df.copy()\n",
        "    # amount zscore per customer\n",
        "    df['amount_z'] = df.groupby('customer_id')['amount'].transform(lambda x: (x - x.mean()) / (x.std().replace(0,1)))\n",
        "\n",
        "    # fraction of declined transactions historically (if available)\n",
        "    if 'status' in df.columns:\n",
        "        df['is_declined'] = (df['status'] == 'DECLINED').astype(int)\n",
        "        df['decline_rate'] = df.groupby('customer_id')['is_declined'].transform('mean')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "6VCUmOKwHVgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Device Features\n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "def device_ip_features(df):\n",
        "    \"\"\"Simple device/IP aggregation features and rare-value flags.\"\"\"\n",
        "    df = df.copy()\n",
        "    # counts per device / ip\n",
        "    if 'device_id' in df.columns:\n",
        "        df['device_txn_count'] = df.groupby('device_id')['transaction_id'].transform('count')\n",
        "        df['unique_customers_per_device'] = df.groupby('device_id')['customer_id'].transform('nunique')\n",
        "        df['device_rare_flag'] = (df['device_txn_count'] < 3).astype(int)\n",
        "\n",
        "    if 'ip_address' in df.columns:\n",
        "        df['ip_txn_count'] = df.groupby('ip_address')['transaction_id'].transform('count')\n",
        "        df['unique_customers_per_ip'] = df.groupby('ip_address')['customer_id'].transform('nunique')\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "1N17_TDkHhnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Missing values and encoding\n",
        "# -----------------------------\n",
        "\n",
        "# CatBoost handles NaNs natively. For features where NaN is informative,\n",
        "# create a missing indicator. For other numeric missingness, you can impute.\n",
        "\n",
        "def add_missing_indicators(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[f'{c}_missing'] = df[c].isna().astype(int)\n",
        "    return df"
      ],
      "metadata": {
        "id": "-IbnPfD3Hj4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# End-to-end feature builder\n",
        "# -----------------------------\n",
        "\n",
        "def build_features(df):\n",
        "    df = create_base_features(df)\n",
        "    df = create_rolling_features(df)\n",
        "    df = device_ip_features(df)\n",
        "    df = behavioral_features(df)\n",
        "\n",
        "    # Missing indicators for key fields\n",
        "    df = add_missing_indicators(df, ['amount','device_id','ip_address','customer_id'])\n",
        "\n",
        "    # Reduce cardinality for very high-cardinality cats (hashing or top-k)\n",
        "    for c in ['device_id','ip_address']:\n",
        "        if c in df.columns:\n",
        "            top = df[c].value_counts().nlargest(1000).index\n",
        "            df[c] = df[c].where(df[c].isin(top), 'OTHER')\n",
        "\n",
        "    # Final fill for any remaining nulls (CatBoost accepts NaN but some downstream ops may not)\n",
        "    df.fillna(np.nan, inplace=True)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Tj-trYJjHqkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Modeling with CatBoost (imbalance + missing values)\n",
        "# -----------------------------\n",
        "\n",
        "def train_catboost(X, y, categorical_features, n_splits=5):\n",
        "    params = dict(\n",
        "        iterations=2000,\n",
        "        depth=6,\n",
        "        learning_rate=0.03,\n",
        "        loss_function='Logloss',\n",
        "        eval_metric='AUC',\n",
        "        l2_leaf_reg=3.0,\n",
        "        random_seed=42,\n",
        "        od_type='Iter',\n",
        "        od_wait=200,\n",
        "        task_type='CPU',\n",
        "        verbose=100\n",
        "    )\n",
        "\n",
        "    # class weights (simple heuristic) - tune on validation\n",
        "    pos = y.mean()\n",
        "    if pos > 0:\n",
        "        params['class_weights'] = [1.0, (1-pos)/pos]\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    oof_proba = np.zeros(len(X))\n",
        "    models = []\n",
        "\n",
        "    for tr, va in cv.split(X, y):\n",
        "        train_pool = Pool(X.iloc[tr], y.iloc[tr], cat_features=categorical_features)\n",
        "        valid_pool = Pool(X.iloc[va], y.iloc[va], cat_features=categorical_features)\n",
        "        model = CatBoostClassifier(**params)\n",
        "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
        "        oof_proba[va] = model.predict_proba(valid_pool)[:,1]\n",
        "        models.append(model)\n",
        "\n",
        "    print('CV AUROC:', roc_auc_score(y, oof_proba))\n",
        "    print('CV AUPRC:', average_precision_score(y, oof_proba))\n",
        "    return models, oof_proba\n"
      ],
      "metadata": {
        "id": "TBFM4Xe7HxQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5EUVx7XGYuw"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Threshold selection & cost-based optimization\n",
        "# -----------------------------\n",
        "\n",
        "def choose_threshold(p, y, tp_gain=100, fp_cost=10, fn_cost=200):\n",
        "    thresholds = np.linspace(0.0005, 0.9995, 2000)\n",
        "    best_th, best_util = 0.5, -1e12\n",
        "    for th in thresholds:\n",
        "        pred = (p >= th).astype(int)\n",
        "        tp = ((pred==1)&(y==1)).sum()\n",
        "        fp = ((pred==1)&(y==0)).sum()\n",
        "        fn = ((pred==0)&(y==1)).sum()\n",
        "        util = tp*tp_gain - fp*fp_cost - fn*fn_cost\n",
        "        if util > best_util:\n",
        "            best_th, best_util = th, util\n",
        "    return best_th, best_util\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Improving detection over time\n",
        "# -----------------------------\n",
        "# 1) Continuous labeling pipeline: record outcomes for flagged transactions (chargebacks, confirmed frauds).\n",
        "# 2) Shadow mode and backtesting: run new models in parallel and compare decisions to production.\n",
        "# 3) Monitoring: track AUPRC, precision@k, false positive rate, and PSI for features.\n",
        "# 4) Retraining triggers: automated retrain when key metrics degrade beyond threshold (e.g., AUPRC drop > 5%).\n",
        "# 5) Incremental training: accumulate new labeled batches and retrain or fine-tune weekly/bi-weekly/monthly depending on drift.\n",
        "\n",
        "\n",
        "# Frequency guidance:\n",
        "# - High-velocity ecommerce (many daily changes, frequent new payment methods): retrain weekly or bi-weekly.\n",
        "# - Moderate traffic: monthly retrain is common.\n",
        "# - Low-volume or very stable domains: quarterly retrain may suffice.\n",
        "# Always use monitoring to adapt frequency (data drift, feature distribution changes, drop in business KPIs).\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# MLOps & deployment notes\n",
        "# -----------------------------\n",
        "# - Use MLflow to track experiments, metrics, and artifacts.\n",
        "# - Use Airflow to orchestrate data ingestion, feature engineering, training, evaluation, and model promotion.\n",
        "# - Deploy models behind a feature flag: shadow -> canary -> full rollout.\n",
        "# - Monitor post-deployment metrics and have automatic rollback rules.\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Example: retrain trigger (pseudo-code)\n",
        "# -----------------------------\n",
        "# if current_auprc < baseline_auprc * 0.95:\n",
        "#     trigger_retraining()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Usage example (assuming df, y available):\n",
        "# -----------------------------\n",
        "# df_feat = build_features(df)\n",
        "# categorical_features = ['interest_category','device_id','ip_address','country']\n",
        "# models, oof = train_catboost(df_feat[categorical_features + numeric_cols], y, categorical_features)\n",
        "# th, util = choose_threshold(oof, y)\n",
        "# print('Selected thresh:', th)\n"
      ],
      "metadata": {
        "id": "1mCe2s_mGbb7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}